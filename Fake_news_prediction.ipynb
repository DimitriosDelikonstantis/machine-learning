{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brown-garbage",
   "metadata": {},
   "source": [
    "The dataset used for this assignment was retrieved [here](https://www.kaggle.com/c/fake-news/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesbian-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset in a dataframe\n",
    "# Pandas.read_csv reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes.\n",
    "Dataframe = pd.read_csv(r'C:\\Users\\dimde\\Documents\\University of Piraeus - MSc in Artificial Intelligence\\Courses\\First semester\\Machine learning\\Assignments\\Machine learning\\Fake news\\Dataset\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-collectible",
   "metadata": {},
   "source": [
    "Reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offensive-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe shape\n",
    "# Returns a tuple representing the dimensionality of the Dataframe\n",
    "Dataframe.shape\n",
    "print(Dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-stuart",
   "metadata": {},
   "source": [
    "Outputs the dimensionality of the dataframe. Our dataset has 5 features and 20800 feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text  label  \n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1      Ever get the feeling your life circles the rou...      0  \n",
      "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
      "...                                                  ...    ...  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
      "20796  When the Green Bay Packers lost to the Washing...      0  \n",
      "20797  The Macy’s of today grew from the union of sev...      0  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
      "20799    David Swanson is an author, activist, journa...      1  \n",
      "\n",
      "[20800 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe head\n",
    "# Returns the first and last 5 rows of the Dataframe\n",
    "Dataframe.head()\n",
    "print(Dataframe.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-positive",
   "metadata": {},
   "source": [
    "Above we can take a look on the first and last 5 feature vectors of the dataset.\n",
    "\n",
    "The 5 features are: **id, title, author, text, label.**\n",
    "\n",
    "**id**: indicates the index of the article (from 0 to 20799, in total 20800 feature vectors).\n",
    "\n",
    "**title**: indicates the title of the article.\n",
    "\n",
    "**author**: indicates the author of the article.\n",
    "\n",
    "**text**: indicates the actual main body of the article.\n",
    "\n",
    "**label**: indicates if the article is fake or not. Value is 0 if the article represents real information and value is 1 if the article represents fake information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-ridge",
   "metadata": {},
   "source": [
    "The features that are going to be examined are the features **label** and **text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precious-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 0, 1 labels to 'REAL' and 'FAKE' for simplicity\n",
    "# With Dataframe.loc set value for an entire column\n",
    "Dataframe.loc[(Dataframe['label'] == 1) , ['label']] = 'FAKE'\n",
    "Dataframe.loc[(Dataframe['label'] == 0) , ['label']] = 'REAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pharmacy",
   "metadata": {},
   "source": [
    "For simplicity's sake convert feature label values \"0\" to \"REAL\" and \"1\" to \"FAKE\". This is only done for comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turkish-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text label  \n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...  FAKE  \n",
      "1      Ever get the feeling your life circles the rou...  REAL  \n",
      "2      Why the Truth Might Get You Fired October 29, ...  FAKE  \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...  FAKE  \n",
      "4      Print \\nAn Iranian woman has been sentenced to...  FAKE  \n",
      "...                                                  ...   ...  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...  REAL  \n",
      "20796  When the Green Bay Packers lost to the Washing...  REAL  \n",
      "20797  The Macy’s of today grew from the union of sev...  REAL  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...  FAKE  \n",
      "20799    David Swanson is an author, activist, journa...  FAKE  \n",
      "\n",
      "[20800 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(Dataframe.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-vector",
   "metadata": {},
   "source": [
    "Now the feature label presents the 0 values with \"REAL\" and the 1 values with \"FAKE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "german-tuition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 0        FAKE\n",
      "1        REAL\n",
      "2        FAKE\n",
      "3        FAKE\n",
      "4        FAKE\n",
      "         ... \n",
      "20795    REAL\n",
      "20796    REAL\n",
      "20797    REAL\n",
      "20798    FAKE\n",
      "20799    FAKE\n",
      "Name: label, Length: 20800, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "# Isolate the feature label from the rest of the dataframe\n",
    "labels = Dataframe.label\n",
    "labels.head()\n",
    "print(labels.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-indonesian",
   "metadata": {},
   "source": [
    "Isolate the label feature to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "#Test for different case scenarios\n",
    "# Test 1 -> 60% train, 40% test, random_state = 7 -> Accuracy: 95.82%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.4, random_state = 7)\n",
    "# Test 2 -> 65% train, 35% test, random_state = 7 -> Accuracy: 96.17%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.35, random_state = 7)\n",
    "# Test 3 -> 70% train, 30% test, random_state = 7 -> Accuracy: 95.99%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.3, random_state = 7)\n",
    "# Test 4 -> 75% train, 25% test, random_state = 7 -> Accuracy: 96.21%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.25, random_state = 7)\n",
    "# Test 5 -> 80% train, 20% test, random_state = 7 -> Accuracy: 96.56%\n",
    "x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.2, random_state = 7)\n",
    "# Test 6 -> 85% train, 15% test, random_state = 7 -> Accuracy: 96.19%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.15, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-belly",
   "metadata": {},
   "source": [
    "The sklearn **train_test_split** function will be used for spliting the dataset.\n",
    "\n",
    "The reason we split the dataset is because we can't use the same data for prediction that we used for training. If we do this then our prediction evaluation will be biased. We need to evaluate our prediction based on \"unseen\" data by the model.\n",
    "\n",
    "In order to have an unbiased prediction evaluation, spliting the dataset is essential. The dataset is also shuffled before applying the split. Furthermore, It is randomized during spliting.\n",
    "\n",
    "As can be seen above, different case scenarios were used for training, testing and spliting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-married",
   "metadata": {},
   "source": [
    "## train_test_split parameters\n",
    "\n",
    "**train_size**: is the number that defines the size of the training set.\n",
    "\n",
    "**test_size**: is the number that defines the size of the test set.\n",
    "\n",
    "**random_state**: is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n",
    "\n",
    "**shuffle**: is the object (**Τrue by default**) that determines whether to shuffle the dataset before applying the split.\n",
    "\n",
    "**stratify**: is an array-like object that, if not None, determines how to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-module",
   "metadata": {},
   "source": [
    "## Bag-of-words model\n",
    "The **bag-of-words** model is a method of representing text data when modeling text with machine learning algorithms. It is easy to comprehend and implement and has seen huge success in applications such as language modeling and document classification.\n",
    "\n",
    "An issue with modeling text is that it is disorganized, and methods like machine learning algorithms fancy defined fixed-length inputs and outputs. Since machine learning algorithms cannot operate with text directly, the text must be changed into vectors of numbers. Therefore we do feature encoding with the bag-of-words model of text. It is a well liked and plain method of feature encoding with text.\n",
    "\n",
    "The method is simple and adaptable and can be used in a lot of ways for pulling out features from documents. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "* A vocabulary of known words.\n",
    "* A measure of the presence of known words.\n",
    "\n",
    "The order of occurrence is not important. We only care about whether known words occur in the document. The bag-of-words approach, looks at the histogram of the words within the text.\n",
    "\n",
    "The objective is to transform each document of free text into a vector that we can use as input or output for a machine learning model. The simplest scoring method is to mark the presence of words as 0 for absent, and 1 for present. New documents that overlap with the vocabulary of known words, but may include words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "In this assignment each article is a \"document\" and all of the articles together are the entire corpus of \"documents\". An entire corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "For a very large corpus, such as thousands of books, the length of the vector might be thousands or millions of positions. Furthermore, each document may include very few of the known words in the vocabulary. This results in a vector with lots of zero scores, called a sparse vector or sparse representation. Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms. Therefore, there is need to decrease the size of the vocabulary when using a bag-of-words model.\n",
    "\n",
    "There are some techniques that can be used as a first step to reduce the size of the vocabulary. For example:\n",
    "* ignoring uppercase and lowercase in words.\n",
    "* ignoring punctuation.\n",
    "* ignoring frequent words that don’t contain much information (called stop words) like \"a\", \"of\", \"him\", \"the\".\n",
    "* fixing misspelled words.\n",
    "* reducing words to their stem using stemming algorithms (for example \"use\" from \"using\").\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This changes the scope of the vocabulary and also allows the bag-of-words to capture somewhat more meaning from the document. By doing this, each word or token is called a **gram**. Creating a vocabulary of two-word pairs is called a **2-gram model** or a **bigram model**. As a result, only the bigrams that appear in the corpus are modeled, not all possible bigrams. A **3-gram model** or a **trigram** is a three-word sequence of words. The general approach is called the **n-gram model**, where n refers to the number of grouped words. Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.\n",
    "\n",
    "Once a vocabulary method has been chosen, the occurrence of words in the documents needs to be scored. As mentioned earlier, the simplest scoring method is **binary scoring** which marks the presence of words as 0 for absent, and 1 for present. Another scoring method is **counts**, which counts the number of times each word appears in a document. Additionally, **frequencies** scoring method calculates the frequency that each word appears in a document out of all the words in the document.\n",
    "\n",
    "Feature hashing is when known words use a hash representation in the vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because then the size of the hash space can be defined, which is in turn the size of the vector representation of the document. Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n",
    "\n",
    "## TF-IDF\n",
    "A problem with scoring word frequency is that highly frequent words start to dominate in the document (for example, larger score), but may not contain as much \"information\" to the model as more rare but perhaps domain specific words.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in entire corpus of documents, so that the scores for frequent words like \"the\" that are also frequent across all documents are penalized. This approach to scoring is called \"Term Frequency–Inverse Document Frequency\", or TF-IDF for short, where:\n",
    "* Term Frequency is a scoring of the frequency of the word in the current document.\n",
    "* Inverse Document Frequency is a scoring of how rare the word is across documents.\n",
    "\n",
    "The scores are a weighting where not all words are equally as important or interesting. The scores have the effect of highlighting words that are distinct (contain useful information) in a given document. Thus, the IDF of a rare term is high, whereas the IDF of a frequent term is likely to be low.\n",
    "\n",
    "The bag-of-words model is very simple to understand and implement and offers a lot of flexibility for customization on specific text data. It has been used with great success on prediction problems like language modeling and documentation classification. Nevertheless, it suffers from some drawbacks, such as:\n",
    "* Vocabulary: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
    "* Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "* Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” as opposed to “is this interesting”), synonyms (“old bike” vs “used bike”), and many more semantics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-preservation",
   "metadata": {},
   "source": [
    "## TfidfVectorizer parameters\n",
    "\n",
    "Declare a TfidfVectorizer using stop words from the English language and allow up to an article frequency of 0.7.\n",
    "\n",
    "**stop_words**: If a string, it is passed to _check_stop_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value. There are several known issues with ‘english’. See below.\n",
    "\n",
    "**Using stop words**\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\n",
    "\n",
    "There are several known issues in our provided ‘english’ stop word list. It does not aim to be a general, ‘one-size-fits-all’ solution as some tasks may require a more custom solution.\n",
    "\n",
    "Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer.\n",
    "\n",
    "You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we’ve is split into we and ve by CountVectorizer’s default tokenizer, so if we’ve is in stop_words, but ve is not, ve will be retained from we’ve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.\n",
    "\n",
    "**max_df**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range (0.0 , 1.0), the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & transform train set, transform test set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-bacon",
   "metadata": {},
   "source": [
    "Fit and transform the TfidfVectorizer on the training set and also transform it on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-emphasis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the PassiveAggressiveClassifier and fit training sets\n",
    "pa_classifier = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pa_classifier.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-bhutan",
   "metadata": {},
   "source": [
    "Initialize the PassiveAggressiveClassifier.\n",
    "Incorporate it into the model by using the “y_train” and “tfidf_train”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and calculate accuracy\n",
    "y_pred = pa_classifier.predict(tfidf_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-mexico",
   "metadata": {},
   "source": [
    "Use the vectorizer to predict whether an article is real or fake and calculate the model’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-commonwealth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "Conf_matrix = confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL'])\n",
    "print('Confusion matrix: ' '\\n', Conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-shepherd",
   "metadata": {},
   "source": [
    "Build confusion matrix to check the successfull predictions and failures.\n",
    "\n",
    "Confusion matrix results:\n",
    "* Positives           FalsePositives\n",
    "* FalseNegatives      Negatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
