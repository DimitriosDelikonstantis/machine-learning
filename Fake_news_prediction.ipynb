{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesbian-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset in a dataframe\n",
    "# Pandas.read_csv reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes.\n",
    "dataframe = pd.read_csv(r'C:\\Users\\dimde\\Documents\\University of Piraeus - MSc in Artificial Intelligence\\Courses\\First semester\\Machine learning\\Assignments\\Machine learning\\Fake news\\Dataset\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-collectible",
   "metadata": {},
   "source": [
    "Reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offensive-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:  (20800, 5) \n",
      "\n",
      "Dataframe axes:  [RangeIndex(start=0, stop=20800, step=1), Index(['id', 'title', 'author', 'text', 'label'], dtype='object')] \n",
      "\n",
      "Dataframe dtypes:  id         int64\n",
      "title     object\n",
      "author    object\n",
      "text      object\n",
      "label      int64\n",
      "dtype: object \n",
      "\n",
      "Dataframe size:  104000 \n",
      "\n",
      "   id                                              title  \\\n",
      "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2   2                  Why the Truth Might Get You Fired   \n",
      "3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4   4  Iranian woman jailed for fictional unpublished...   \n",
      "5   5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
      "6   6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
      "7   7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
      "8   8  Excerpts From a Draft Script for Donald Trump’...   \n",
      "9   9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
      "\n",
      "                         author  \\\n",
      "0                 Darrell Lucus   \n",
      "1               Daniel J. Flynn   \n",
      "2            Consortiumnews.com   \n",
      "3               Jessica Purkiss   \n",
      "4                Howard Portnoy   \n",
      "5               Daniel Nussbaum   \n",
      "6                           NaN   \n",
      "7               Alissa J. Rubin   \n",
      "8                           NaN   \n",
      "9  Megan Twohey and Scott Shane   \n",
      "\n",
      "                                                text  label  \n",
      "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1  Ever get the feeling your life circles the rou...      0  \n",
      "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4  Print \\nAn Iranian woman has been sentenced to...      1  \n",
      "5  In these trying times, Jackie Mason is the Voi...      0  \n",
      "6  Ever wonder how Britain’s most iconic pop pian...      1  \n",
      "7  PARIS  —   France chose an idealistic, traditi...      0  \n",
      "8  Donald J. Trump is scheduled to make a highly ...      0  \n",
      "9  A week before Michael T. Flynn resigned as nat...      0  \n"
     ]
    }
   ],
   "source": [
    "# Get dataframe information\n",
    "# Represents the dimensionality of the Dataframe\n",
    "print('Dataframe shape: ', dataframe.shape, '\\n')\n",
    "# Represents the axes of the Dataframe\n",
    "print('Dataframe axes: ', dataframe.axes, '\\n')\n",
    "# Returns the dtypes in the Dataframe\n",
    "print('Dataframe dtypes: ', dataframe.dtypes, '\\n')\n",
    "# Returns an int representing the number of elements in this object\n",
    "print('Dataframe size: ', dataframe.size, '\\n')\n",
    "# Returns the first 10 rows of the Dataframe\n",
    "print(dataframe.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-positive",
   "metadata": {},
   "source": [
    "Above we retrieve some information of the dataframe.\n",
    "\n",
    "Firstly, we retrieve the dimensionality of the dataframe. Our dataset has 5 features and 20800 samples.\n",
    "\n",
    "Then, we get the features of the dataframe. The 5 features are 'id', 'title', 'author', 'text' and 'label'.\n",
    "\n",
    "Additionally, we extract the dataframe feature sample types (object).\n",
    "\n",
    "Furthermore, we retrieve the dataframe size. This is the number of elements in the dataframe.\n",
    "\n",
    "Last but not least, with dataframe.head we view the first 10 rows of the dataframe for analyzation.\n",
    "\n",
    "Feature description:\n",
    "\n",
    "* **id**: indicates the index of the article (from 0 to 20799, in total 20800 feature vectors/samples).\n",
    "\n",
    "* **title**: indicates the title of the article.\n",
    "\n",
    "* **author**: indicates the author of the article.\n",
    "\n",
    "* **text**: indicates the actual main body of the article.\n",
    "\n",
    "* **label**: indicates if the article is fake or not. Value is 0 if the article represents real information and value is 1 if the article represents fake information.\n",
    "\n",
    "The features that are going to be examined are the features **label** and **text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "precious-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 0, 1 labels to 'REAL' and 'FAKE' for simplicity\n",
    "# With Dataframe.loc set value for an entire column\n",
    "dataframe.loc[(dataframe['label'] == 1) , ['label']] = 'FAKE'\n",
    "dataframe.loc[(dataframe['label'] == 0) , ['label']] = 'REAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pharmacy",
   "metadata": {},
   "source": [
    "For our better visualization convert feature label values \"0\" to \"REAL\" and \"1\" to \"FAKE\". This is only done for comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turkish-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text label  \n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...  FAKE  \n",
      "1      Ever get the feeling your life circles the rou...  REAL  \n",
      "2      Why the Truth Might Get You Fired October 29, ...  FAKE  \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...  FAKE  \n",
      "4      Print \\nAn Iranian woman has been sentenced to...  FAKE  \n",
      "...                                                  ...   ...  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...  REAL  \n",
      "20796  When the Green Bay Packers lost to the Washing...  REAL  \n",
      "20797  The Macy’s of today grew from the union of sev...  REAL  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...  FAKE  \n",
      "20799    David Swanson is an author, activist, journa...  FAKE  \n",
      "\n",
      "[20800 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "#Visualize the head again to check the changes\n",
    "print(dataframe.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-vector",
   "metadata": {},
   "source": [
    "Now the feature label presents the 0 values with \"REAL\" and the 1 values with \"FAKE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "german-tuition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 0        FAKE\n",
      "1        REAL\n",
      "2        FAKE\n",
      "3        FAKE\n",
      "4        FAKE\n",
      "         ... \n",
      "20795    REAL\n",
      "20796    REAL\n",
      "20797    REAL\n",
      "20798    FAKE\n",
      "20799    FAKE\n",
      "Name: label, Length: 20800, dtype: object>\n",
      "<bound method NDFrame.head of 0        House Dem Aide: We Didn’t Even See Comey’s Let...\n",
      "1        Ever get the feeling your life circles the rou...\n",
      "2        Why the Truth Might Get You Fired October 29, ...\n",
      "3        Videos 15 Civilians Killed In Single US Airstr...\n",
      "4        Print \\nAn Iranian woman has been sentenced to...\n",
      "                               ...                        \n",
      "20795    Rapper T. I. unloaded on black celebrities who...\n",
      "20796    When the Green Bay Packers lost to the Washing...\n",
      "20797    The Macy’s of today grew from the union of sev...\n",
      "20798    NATO, Russia To Hold Parallel Exercises In Bal...\n",
      "20799      David Swanson is an author, activist, journa...\n",
      "Name: text, Length: 20800, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "# Isolate the label feature from the rest of the dataframe\n",
    "labels = dataframe.label\n",
    "labels.head()\n",
    "print(labels.head)\n",
    "text = dataframe.text\n",
    "text.head()\n",
    "print(text.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-indonesian",
   "metadata": {},
   "source": [
    "Isolate the label feature from the dataframe so that we use it for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ruled-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "#Test for different case scenarios\n",
    "# Test 1 -> 60% train, 40% test, random_state = 7 -> Accuracy: 95.82%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.4, random_state = 7)\n",
    "# Test 2 -> 65% train, 35% test, random_state = 7 -> Accuracy: 96.17%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.35, random_state = 7)\n",
    "# Test 3 -> 70% train, 30% test, random_state = 7 -> Accuracy: 95.99%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.3, random_state = 7)\n",
    "# Test 4 -> 75% train, 25% test, random_state = 7 -> Accuracy: 96.21%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.25, random_state = 7)\n",
    "# Test 5 -> 80% train, 20% test, random_state = 7 -> Accuracy: 96.56%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.2, random_state = 7)\n",
    "# Test 6 -> 85% train, 15% test, random_state = 7 -> Accuracy: 96.19%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.15, random_state = 7)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(text, labels, test_size = 0.2, random_state = 7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-belly",
   "metadata": {},
   "source": [
    "The sklearn **train_test_split** function will be used for spliting the dataset.\n",
    "\n",
    "The reason we split the dataset is because we can't use the same data for prediction that we used for training. If we do this then our prediction evaluation will be biased. We need to evaluate our prediction based on \"unseen\" data by the model.\n",
    "\n",
    "In order to have an unbiased prediction evaluation, spliting the dataset is essential. The dataset is also shuffled before applying the split. Furthermore, It is randomized during spliting.\n",
    "\n",
    "As can be seen above, different case scenarios were used for training, testing and spliting the data. Highest accuracy scenario was using 80% of the dataset for train and 20% of the dataset for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-married",
   "metadata": {},
   "source": [
    "## train_test_split parameters\n",
    "\n",
    "**train_size**: is the number that defines the size of the training set.\n",
    "\n",
    "**test_size**: is the number that defines the size of the test set.\n",
    "\n",
    "**random_state**: is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n",
    "\n",
    "**shuffle**: is the object (**Τrue by default**) that determines whether to shuffle the dataset before applying the split.\n",
    "\n",
    "**stratify**: is an array-like object that, if not None, determines how to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "personalized-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words list\n",
    "stop_words_list = ['a, the, of, this, that', 'him', 'an',]\n",
    "\n",
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-module",
   "metadata": {},
   "source": [
    "## Bag-of-words model\n",
    "The **bag-of-words** model is a method of representing text data when modeling text with machine learning algorithms. It is easy to comprehend and implement and has seen huge success in applications such as language modeling and document classification.\n",
    "\n",
    "An issue with modeling text is that it is disorganized, and methods like machine learning algorithms fancy defined fixed-length inputs and outputs. Since machine learning algorithms cannot operate with text directly, the text must be changed into vectors of numbers. Therefore we do feature encoding with the bag-of-words model of text. It is a well liked and plain method of feature encoding with text.\n",
    "\n",
    "The method is simple and adaptable and can be used in a lot of ways for pulling out features from documents. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "* A vocabulary of known words.\n",
    "* A measure of the presence of known words.\n",
    "\n",
    "The order of occurrence is not important. We only care about whether known words occur in the document. The bag-of-words approach, looks at the histogram of the words within the text.\n",
    "\n",
    "The objective is to transform each document of free text into a vector that we can use as input or output for a machine learning model. The simplest scoring method is to mark the presence of words as 0 for absent, and 1 for present. New documents that overlap with the vocabulary of known words, but may include words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "In this assignment each article is a \"document\" and all of the articles together are the entire corpus of \"documents\". An entire corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "For a very large corpus, such as thousands of books, the length of the vector might be thousands or millions of positions. Furthermore, each document may include very few of the known words in the vocabulary. This results in a vector with lots of zero scores, called a sparse vector or sparse representation. Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms. Therefore, there is need to decrease the size of the vocabulary when using a bag-of-words model.\n",
    "\n",
    "There are some techniques that can be used as a first step to reduce the size of the vocabulary. For example:\n",
    "* ignoring uppercase and lowercase in words.\n",
    "* ignoring punctuation.\n",
    "* ignoring frequent words that don’t contain much information (called stop words) like \"a\", \"of\", \"him\", \"the\".\n",
    "* fixing misspelled words.\n",
    "* reducing words to their stem using stemming algorithms (for example \"use\" from \"using\").\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This changes the scope of the vocabulary and also allows the bag-of-words to capture somewhat more meaning from the document. By doing this, each word or token is called a **gram**. Creating a vocabulary of two-word pairs is called a **2-gram model** or a **bigram model**. As a result, only the bigrams that appear in the corpus are modeled, not all possible bigrams. A **3-gram model** or a **trigram** is a three-word sequence of words. The general approach is called the **n-gram model**, where n refers to the number of grouped words. Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.\n",
    "\n",
    "Once a vocabulary method has been chosen, the occurrence of words in the documents needs to be scored. As mentioned earlier, the simplest scoring method is **binary scoring** which marks the presence of words as 0 for absent, and 1 for present. Another scoring method is **counts**, which counts the number of times each word appears in a document. Additionally, **frequencies** scoring method calculates the frequency that each word appears in a document out of all the words in the document.\n",
    "\n",
    "Feature hashing is when known words use a hash representation in the vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because then the size of the hash space can be defined, which is in turn the size of the vector representation of the document. Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n",
    "\n",
    "## TF-IDF\n",
    "A problem with scoring word frequency is that highly frequent words start to dominate in the document (for example, larger score), but may not contain as much \"information\" to the model as more rare but perhaps domain specific words.\n",
    "\n",
    "One approach is to rescale the frequency of words by how often they appear in entire corpus of documents, so that the scores for frequent words like \"the\" that are also frequent across all documents are penalized. This approach to scoring is called \"Term Frequency–Inverse Document Frequency\", or TF-IDF for short, where:\n",
    "* Term Frequency is a scoring of the frequency of the word in the current document.\n",
    "* Inverse Document Frequency is a scoring of how rare the word is across documents.\n",
    "\n",
    "The scores are a weighting where not all words are equally as important or interesting. The scores have the effect of highlighting words that are distinct (contain useful information) in a given document. Thus, the IDF of a rare term is high, whereas the IDF of a frequent term is likely to be low.\n",
    "\n",
    "The bag-of-words model is very simple to understand and implement and offers a lot of flexibility for customization on specific text data. It has been used with great success on prediction problems like language modeling and documentation classification. Nevertheless, it suffers from some drawbacks, such as:\n",
    "* Vocabulary: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n",
    "* Sparsity: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n",
    "* Meaning: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” as opposed to “is this interesting”), synonyms (“old bike” vs “used bike”), and many more semantics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-preservation",
   "metadata": {},
   "source": [
    "## TfidfVectorizer parameters\n",
    "\n",
    "Declare a TfidfVectorizer using stop words from the English language and allow up to an article frequency of 0.7.\n",
    "\n",
    "**stop_words**: If a string, it is passed to _check_stop_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value. There are several known issues with ‘english’. See below.\n",
    "\n",
    "**Using stop words**\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\n",
    "\n",
    "There are several known issues in our provided ‘english’ stop word list. It does not aim to be a general, ‘one-size-fits-all’ solution as some tasks may require a more custom solution.\n",
    "\n",
    "Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer.\n",
    "\n",
    "You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we’ve is split into we and ve by CountVectorizer’s default tokenizer, so if we’ve is in stop_words, but ve is not, ve will be retained from we’ve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.\n",
    "\n",
    "**max_df**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range (0.0 , 1.0), the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "checked-colorado",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c61c6113b28f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit and transform train set and test set to TfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtfidf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtfidf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Vocabulary length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dimde\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dimde\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dimde\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dimde\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dimde\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Fit and transform train set and test set to TfidfVectorizer\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "#Vocabulary length\n",
    "vocabulary = len(tfidf_vectorizer.get_feature_names())\n",
    "print('Length tfidf vectorizer: ', vocabulary)\n",
    "\n",
    "#TfidfVectorizer stop words\n",
    "default_stop_words = tfidf_vectorizer.get_stop_words()\n",
    "print('TfidfVectorizer stop words', default_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-bacon",
   "metadata": {},
   "source": [
    "Fit and transform the TfidfVectorizer on the training set and also transform it on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-emphasis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the PassiveAggressiveClassifier and fit training sets\n",
    "pa_classifier = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pa_classifier.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-bhutan",
   "metadata": {},
   "source": [
    "Initialize the PassiveAggressiveClassifier.\n",
    "Incorporate it into the model by using the “y_train” and “tfidf_train”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and calculate accuracy\n",
    "y_pred = pa_classifier.predict(tfidf_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-mexico",
   "metadata": {},
   "source": [
    "Use the vectorizer to predict whether an article is real or fake and calculate the model’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-commonwealth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build confusion matrix\n",
    "Conf_matrix = confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL'])\n",
    "print('Confusion matrix: ' '\\n', Conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-shepherd",
   "metadata": {},
   "source": [
    "Build confusion matrix to check the successfull predictions and failures.\n",
    "\n",
    "Confusion matrix results:\n",
    "* Positives           FalsePositives\n",
    "* FalseNegatives      Negatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
