{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brown-garbage",
   "metadata": {},
   "source": [
    "The dataset used for this assignment was retrieved [here](https://www.kaggle.com/c/fake-news/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simplified-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesbian-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset in a dataframe\n",
    "# Pandas.read_csv reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes.\n",
    "Dataframe = pd.read_csv(r'C:\\Users\\dimde\\Documents\\University of Piraeus - MSc in Artificial Intelligence\\Courses\\First semester\\Machine learning\\Assignments\\Machine learning\\Fake news\\Dataset\\train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-collectible",
   "metadata": {},
   "source": [
    "Reads a comma-separated values (csv) file into Dataframe and returns a two-dimensional data structure with labeled axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "offensive-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5)\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe shape\n",
    "# Returns a tuple representing the dimensionality of the Dataframe\n",
    "Dataframe.shape\n",
    "print(Dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-stuart",
   "metadata": {},
   "source": [
    "Our dataset has 5 features and 20800 feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text  label  \n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
      "1      Ever get the feeling your life circles the rou...      0  \n",
      "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
      "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
      "...                                                  ...    ...  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
      "20796  When the Green Bay Packers lost to the Washing...      0  \n",
      "20797  The Macy’s of today grew from the union of sev...      0  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
      "20799    David Swanson is an author, activist, journa...      1  \n",
      "\n",
      "[20800 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Get the dataframe head\n",
    "# Returns the first and last 5 rows of the Dataframe\n",
    "Dataframe.head()\n",
    "print(Dataframe.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-positive",
   "metadata": {},
   "source": [
    "Above we can take a look on the first and last 5 feature vectors of the dataset.\n",
    "\n",
    "The 5 features are: **id, title, author, text, label.**\n",
    "\n",
    "**id**: indicates the index of the article (from 0 to 20799, in total 20800 feature vectors).\n",
    "\n",
    "**title**: indicates the title of the article.\n",
    "\n",
    "**author**: indicates the author of the article.\n",
    "\n",
    "**text**: indicates the actual main body of the article.\n",
    "\n",
    "**label**: indicates if the article is fake or not. Value is 0 if the article represents real information and value is 1 if the article represents fake information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-ridge",
   "metadata": {},
   "source": [
    "The features that are going to be examined are the features label and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precious-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 0, 1 labels to 'REAL' and 'FAKE' for simplicity\n",
    "# With Dataframe.loc set value for an entire column\n",
    "Dataframe.loc[(Dataframe['label'] == 1) , ['label']] = 'FAKE'\n",
    "Dataframe.loc[(Dataframe['label'] == 0) , ['label']] = 'REAL'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pharmacy",
   "metadata": {},
   "source": [
    "For simplicity's sake convert feature label values \"0\" to \"REAL\" and \"1\" to \"FAKE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turkish-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           id                                              title  \\\n",
      "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
      "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
      "2          2                  Why the Truth Might Get You Fired   \n",
      "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
      "4          4  Iranian woman jailed for fictional unpublished...   \n",
      "...      ...                                                ...   \n",
      "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
      "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
      "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
      "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
      "20799  20799                          What Keeps the F-35 Alive   \n",
      "\n",
      "                                          author  \\\n",
      "0                                  Darrell Lucus   \n",
      "1                                Daniel J. Flynn   \n",
      "2                             Consortiumnews.com   \n",
      "3                                Jessica Purkiss   \n",
      "4                                 Howard Portnoy   \n",
      "...                                          ...   \n",
      "20795                              Jerome Hudson   \n",
      "20796                           Benjamin Hoffman   \n",
      "20797  Michael J. de la Merced and Rachel Abrams   \n",
      "20798                                Alex Ansary   \n",
      "20799                              David Swanson   \n",
      "\n",
      "                                                    text label  \n",
      "0      House Dem Aide: We Didn’t Even See Comey’s Let...  FAKE  \n",
      "1      Ever get the feeling your life circles the rou...  REAL  \n",
      "2      Why the Truth Might Get You Fired October 29, ...  FAKE  \n",
      "3      Videos 15 Civilians Killed In Single US Airstr...  FAKE  \n",
      "4      Print \\nAn Iranian woman has been sentenced to...  FAKE  \n",
      "...                                                  ...   ...  \n",
      "20795  Rapper T. I. unloaded on black celebrities who...  REAL  \n",
      "20796  When the Green Bay Packers lost to the Washing...  REAL  \n",
      "20797  The Macy’s of today grew from the union of sev...  REAL  \n",
      "20798  NATO, Russia To Hold Parallel Exercises In Bal...  FAKE  \n",
      "20799    David Swanson is an author, activist, journa...  FAKE  \n",
      "\n",
      "[20800 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(Dataframe.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-vector",
   "metadata": {},
   "source": [
    "Now the feature label presents the 0 values with \"REAL\" and the 1 values with \"FAKE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "german-tuition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of 0        FAKE\n",
      "1        REAL\n",
      "2        FAKE\n",
      "3        FAKE\n",
      "4        FAKE\n",
      "         ... \n",
      "20795    REAL\n",
      "20796    REAL\n",
      "20797    REAL\n",
      "20798    FAKE\n",
      "20799    FAKE\n",
      "Name: label, Length: 20800, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "# Isolate the feature label from the rest of the dataframe\n",
    "labels = Dataframe.label\n",
    "labels.head()\n",
    "print(labels.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ruled-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "#Test for different case scenarios\n",
    "# Test 1 -> 60% train, 40% test, random_state = 7 -> Accuracy: 95.82%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.4, random_state = 7)\n",
    "# Test 2 -> 65% train, 35% test, random_state = 7 -> Accuracy: 96.17%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.35, random_state = 7)\n",
    "# Test 3 -> 70% train, 30% test, random_state = 7 -> Accuracy: 95.99%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.3, random_state = 7)\n",
    "# Test 4 -> 75% train, 25% test, random_state = 7 -> Accuracy: 96.21%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.25, random_state = 7)\n",
    "# Test 5 -> 80% train, 20% test, random_state = 7 -> Accuracy: 96.56%\n",
    "x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.2, random_state = 7)\n",
    "# Test 6 -> 85% train, 15% test, random_state = 7 -> Accuracy: 96.19%\n",
    "#x_train,x_test,y_train,y_test = train_test_split(Dataframe['text'].values.astype('str'), labels, test_size = 0.15, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-belly",
   "metadata": {},
   "source": [
    "The sklearn **train_test_split** function will be used for spliting the dataset.\n",
    "\n",
    "The reason we split the dataset is because we can't use the same data for prediction that we used for training. If we do this then our prediction evaluation will be biased. We need to evaluate our prediction based on \"unseen\" data by the model.\n",
    "\n",
    "In order to have an unbiased prediction evaluation, spliting the dataset is essential. The dataset is also shuffled before applying the split. Furthermore, It is randomized during spliting.\n",
    "\n",
    "As can be seen above, different case scenarios were used for training, testing and spliting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-married",
   "metadata": {},
   "source": [
    "**train_test_split parameters**\n",
    "\n",
    "**train_size**: is the number that defines the size of the training set.\n",
    "\n",
    "**test_size**: is the number that defines the size of the test set.\n",
    "\n",
    "**random_state**: is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. The default value is None.\n",
    "\n",
    "**shuffle**: is the object (**Τrue by default**) that determines whether to shuffle the dataset before applying the split.\n",
    "\n",
    "**stratify**: is an array-like object that, if not None, determines how to use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "personalized-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-module",
   "metadata": {},
   "source": [
    "## Bag of words model\n",
    "\n",
    "Since machine learning algorithms cannot work with text directly, the text must be converted into vectors of numbers. Therefore we do feature encoding with the bag-of-words model. The bag of words model is a popular and simple method of feature encoding with text data. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things: a vocabulary of known words and a measure of the presence of known words. The order of occurence is not important. The bag-of-words approach, looks at the histogram of the words within the text. The simplest scoring method is to mark the presence of words as 0 for absent, 1 for present. In our case each article is a \"document\" and all of the articles are the entire corpus of \"documents\". New documents that overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n",
    "\n",
    "An entire corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "For a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary. This results in a vector with lots of zero scores, called a sparse vector or sparse representation. Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-preservation",
   "metadata": {},
   "source": [
    "**TfidfVectorizer parameters**\n",
    "\n",
    "**stop_words**: If a string, it is passed to _check_stop_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value. There are several known issues with ‘english’. See below.\n",
    "\n",
    "**Using stop words**\n",
    "\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction. Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.\n",
    "\n",
    "There are several known issues in our provided ‘english’ stop word list. It does not aim to be a general, ‘one-size-fits-all’ solution as some tasks may require a more custom solution.\n",
    "\n",
    "Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as computer.\n",
    "\n",
    "You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word we’ve is split into we and ve by CountVectorizer’s default tokenizer, so if we’ve is in stop_words, but ve is not, ve will be retained from we’ve in transformed text. Our vectorizers will try to identify and warn about some kinds of inconsistencies.\n",
    "\n",
    "**max_df**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range (0.0 , 1.0), the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "checked-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit & transform train set, transform test set\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train) \n",
    "tfidf_test = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "terminal-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(max_iter=50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the PassiveAggressiveClassifier and fit training sets\n",
    "pa_classifier = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pa_classifier.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adolescent-warren",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.61%\n"
     ]
    }
   ],
   "source": [
    "# Predict and calculate accuracy\n",
    "y_pred = pa_classifier.predict(tfidf_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imposed-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[2033   67]\n",
      " [  74 1986]]\n"
     ]
    }
   ],
   "source": [
    "# Build confusion matrix\n",
    "Conf_matrix = confusion_matrix(y_test, y_pred, labels=['FAKE', 'REAL'])\n",
    "print('Confusion matrix: ' '\\n', Conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
